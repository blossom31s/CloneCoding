{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blossom31s/CloneCoding/blob/master/BAE_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @ Administration"
      ],
      "metadata": {
        "id": "mz2i1f77yjre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "### [ Exam ] ###\n",
        "################\n",
        "\n",
        "'''\n",
        "* Schedule\n",
        "- 8회 : 실기접수 0520(월) / 실기 0622(토) / 결과발표 0712(금)\n",
        "- 9회 : ?\n",
        "\n",
        "* Information\n",
        "- Coursera(수료증) : https://www.coursera.org/browse/data-science\n",
        "- K-MOOC(수료증) : http://www.kmooc.kr\n",
        "- 통계교육원(수료증) : https://sti.kostat.go.kr\n",
        "- Inflearn(수료증) : https://www.inflearn.com\n",
        "- 포항공대(수료증) : https://pabi.smartlearn.io\n",
        "- 한국정보기술연구원 재직자교육(수료증) : https://estudy.kitri.re.kr\n",
        "- CCCR Academy(수료증) : https://www.cccr-edu.or.kr\n",
        "- Data On-Air : https://dataonair.or.kr\n",
        "- DataManim : https://www.datamanim.com\n",
        "- 데이터 사이언스 스쿨 : https://datascienceschool.net\n",
        "- 프로그래밍 무료 학습 : https://m.blog.naver.com/PostView.naver?blogId=cyberpass&logNo=222872533222&proxyReferer=\n",
        "- 빅데이터 분석기사 실기 준비를 위한 캐글 놀이터 : https://www.kaggle.com/datasets/agileteam/bigdatacertificationkr\n",
        "- 데이터 전문가 포럼 : https://cafe.naver.com/sqlpd\n",
        "- 개인 포스팅\n",
        "    Logit&Odds\n",
        "        https://lovelydiary.tistory.com/347\n",
        "        http://bigdata.dongguk.ac.kr/lectures/med_stat/_book/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95.html\n",
        "        https://todayisbetterthanyesterday.tistory.com/11\n",
        "\n",
        "* To do\n",
        "- 실기 체험환경 : https://dataq.goorm.io/exam/3/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC-%EC%8B%A4%EA%B8%B0-%EC%B2%B4%ED%97%98/quiz/1\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dteC_u6HUwjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################\n",
        "### [ Summary ] ###\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "# Concept\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "! Edit\n",
        "\n",
        "\n",
        "\n",
        "* Statistics\n",
        "- Centrality / Dispersion / Asymmetry\n",
        "- Correlation Coefficient\n",
        "- Normalization & Standardization\n",
        "- Statistical Analysis / Framed-Data Analysis\n",
        "- Hypothesis Test / Parametric Significance Test\n",
        "\n",
        "* Code(Python)\n",
        "- Setup\n",
        "- Data Set\n",
        "- Data Number\n",
        "- Data String\n",
        "- Data Time\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# Practice\n",
        "\n",
        "'''\n",
        "* Type I\n",
        "- Setup\n",
        "- DataFrame\n",
        "- Time & String\n",
        "\n",
        "* Type II : Classification / Regression\n",
        "- 1. Load&Check Datasets\n",
        "- 2-1. [ Pre-Processing ] Delete Unnecessary Columns\n",
        "- 2-2. [ Pre-Processing ] Process The Missing Values\n",
        "    : age, fare, cabin, embarked / Position_Class, Height_cm, Weight_lb\n",
        "- 2-3. [ Pre-Processing ] Process The Categorical Types\n",
        "    : sex, ticket / Age, Club, Work_Rate\n",
        "- 2-4. [ Pre-Processing ] Process The Numerical Types\n",
        "    : pclass, sibsp&parch / Jersey_Number, Contract_Valid_Until, Release_Clause\n",
        "- 2-5. [ Pre-Processing ] Data Separation : train_test_split\n",
        "- 2-6. [ Pre-Processing ] Encoding\n",
        "    : OneHotEncoder\n",
        "- 2-7. [ Pre-Processing ] Scaling\n",
        "    : MinMaxScaler / StandardScaler\n",
        "- 2-8. [ Pre-Processing ] Concatenating\n",
        "- 3-1. [ Modeling ] Train\n",
        "    : RandomForestClassifier / RandomForestRegressor\n",
        "- 3-2. [ Modeling ] Validation\n",
        "    : roc_curve, auc / mean_squared_error\n",
        "- 3-3. [ Modeling ] Test\n",
        "- 4. Submit&Check Result\n",
        "- 5(Ext). Result Evaluation\n",
        "\n",
        "* Type III : T-Test & ANOVA\n",
        "- 1. Sample Mean : bm, am, sm\n",
        "- 2. Test Statistics : ttest_1samp, ttest_ind, ttest_rel, f_oneway\n",
        "- 3. Accept or Reject H0 : cv1, cv2, cv3 / anova[0]_cv3, anova[1]_alpha\n",
        "\n",
        "* Type III : Chi^2-Test\n",
        "- 1. Contingency Table : crosstab\n",
        "- 2. Test Statistics : chi2_contingency\n",
        "- 3. Odds : logit()\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ri4Ppv0yiEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @ Setup"
      ],
      "metadata": {
        "id": "Ch3xgScmfVsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9CB3z1Pd7DL"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "### [ Drive Mount ] ###\n",
        "#######################\n",
        "\n",
        "\n",
        "\n",
        "# Link \"Colab\" with \"Google Drive\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "### [ Python - DataSet ] ###\n",
        "############################\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "* Scikit-Learn(15)\n",
        "- Toy Datasets(6)\n",
        "\tload_iris(*[, return_X_y, as_frame]) # (classification).\n",
        "\tload_diabetes(*[, return_X_y, as_frame, scaled]) # (regression).\n",
        "\tload_digits(*[, n_class, return_X_y, as_frame]) # (classification).\n",
        "\tload_linnerud(*[, return_X_y, as_frame])\n",
        "\tload_wine(*[, return_X_y, as_frame]) # (classification).\n",
        "\tload_breast_cancer(*[, return_X_y, as_frame]) # (classification).\n",
        "- Real World Datasets(9)\n",
        "\tfetch_olivetti_faces(*[, data_home, ...]) # (classification).\n",
        "\tfetch_20newsgroups(*[, data_home, subset, ...]) # (classification).\n",
        "\tfetch_20newsgroups_vectorized(*[, subset, ...]) # (classification).\n",
        "\tfetch_lfw_people(*[, data_home, funneled, ...]) # (classification).\n",
        "\tfetch_lfw_pairs(*[, subset, data_home, ...]) # (classification).\n",
        "\tfetch_covtype(*[, data_home, ...]) # (classification).\n",
        "\tfetch_rcv1(*[, data_home, subset, ...]) # (classification).\n",
        "\tfetch_kddcup99(*[, subset, data_home, ...]) # (classification).\n",
        "\tfetch_california_housing(*[, data_home, ...]) # (regression).\n",
        "\n",
        "* Seaborn(22)\n",
        "- anagrams(20 × 5)\n",
        "- anscombe(44 × 3)\n",
        "- attention(60 × 5)\n",
        "- brain_networks(923 × 63)\n",
        "- car_crashes(51 × 8)\n",
        "- diamonds(53940 × 10)\n",
        "- dots(848 × 5)\n",
        "- dowjones(649 × 2)\n",
        "- exercise(90 × 6)\n",
        "- flights(144 × )\n",
        "- fmri(1064 × 5)\n",
        "- geyser(272 × 3)\n",
        "- glue(64 × 5)\n",
        "- healthexp(274 × 4)\n",
        "- iris(150 × 5)\n",
        "- mpg(398 × 9)\n",
        "- penguins(344 × 7)\n",
        "- planets(1035 × 6)\n",
        "- seaice(13175 × 2)\n",
        "- taxis(6433 × 14)\n",
        "- tips(244 × 7)\n",
        "- titanic(891 × 15)\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xwmomeOS37XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "### [ Data Path ] ###\n",
        "#####################\n",
        "\n",
        "\n",
        "\n",
        "# Absolute Path\n",
        "df = pd.read_csv('/content/drive/MyDrive/BAE/RDatasets/iris.csv')\n",
        "\n",
        "# Relative Path\n",
        "os.chdir('/content/drive/MyDrive/BAE/RDatasets')\n",
        "os.getcwd()\n",
        "df = pd.read_csv('./iris.csv')\n",
        "\n",
        "# Variable Path\n",
        "dataFolder = '/content/drive/MyDrive/BAE/RDatasets'\n",
        "df = pd.read_csv(dataFolder + '/iris.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6swJL4Da37gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "### [ Convenience ] ###\n",
        "#######################\n",
        "\n",
        "\n",
        "\n",
        "# Manual\n",
        "help(pd.DataFrame.agg)\n",
        "dir(sklearn)\n",
        "\n",
        "# Display\n",
        "pd.set_option('display.max_rows', 10) # Limit\n",
        "pd.set_option('display.max_columns', None) # No Limit\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g2p41esA-h5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @ Concept"
      ],
      "metadata": {
        "id": "zVVaQP34E4Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################\n",
        "### [ Package ] ###\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "* Data Process\n",
        "- DataSet\n",
        "\timport seaborn as sns\n",
        "- Data Edit\n",
        "\timport numpy as np\n",
        "\timport pandas as pd\n",
        "\n",
        "* Data Analysis\n",
        "- Preprocessing\n",
        "\tfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\tfrom sklearn.feature_selection\n",
        "\tfrom sklearn.feature_extraction\n",
        "- Train&Test Preparation\n",
        "\tfrom sklearn.model_selection import train_test_split\n",
        "- Machine Learning\n",
        "\tSupervised Learning\n",
        "\t\tfrom sklearn.tree import DesicionTree_Classifier&Regressor\n",
        "\t\tfrom sklearn.ensemble import RandomForest_Classifier&Regressor\n",
        "\t\t\t, GradientBoosting_Classifier&Regressor\n",
        "\t\tfrom sklearn.neighbors import KNeighborsClassifier\n",
        "\t\tfrom sklearn.svm import SVC\n",
        "\t\t\tParameter : C(비용), Gamma(허용 표준편차)\n",
        "\t\t\t\tsvm.SVC(kernel = 'rbf', C = 1, gamma = 0.1)\n",
        "\t\t\t\tsvm.SVC(kernel = 'linear', C = 0.1, gamma = 0.1)\n",
        "\t\tfrom sklearn.linear_model import LogisticRegression(Classification), Ridge, Lasso\n",
        "\t\t\t, LinearRegression\n",
        "\t\tfrom sklearn.naive_bayes import GaussianNB\n",
        "\tUnsupervised Learning\n",
        "\t\tfrom sklearn.cluster import KMeans\n",
        "\t\tfrom mlxtend.frequent import apriori, association_rules\n",
        "- Performance Evaluation\n",
        "\tClassification\n",
        "\t\tfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\tRegression\n",
        "\t\tfrom sklearn.metrics import mean_squared_error, r2_score\n",
        "- Visualization\n",
        "\timport matplotlib.pyplot as plt\n",
        "\tfrom mpl_toolkits.mplot3d import Axes3D\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6iQbMmhqKjta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "### [ Type_1 ] ###\n",
        "##################\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "* Data Edit\n",
        "- DataType\n",
        "\tNumber, String, List[], Tuple(), Dictionary{}, Set{}, Boolean / NDArray, Series, DataFrame\n",
        "\t동일한 자료형만 담을 수 있는 NDArray ≠ 다양한 자료형을 담을 수 있는 List\n",
        "- Operator\n",
        "\t종류\n",
        "\t\t산술(+, -, *, /, //, %, **)\n",
        "\t\t비교(>, <, >=, <=, ==, !=)\n",
        "\t\t논리(and, or, not)\n",
        "\t\t복합대입(+=, -=, *=, /=) : \"a += 10\" == \"a = a + 10\"\n",
        "\t우선순위\n",
        "\t\t() > ** > 부호(+, -) > *, /, //, % > +, - > 비교(관계) > not > and > or\n",
        "- Function(numpy as np(NDArray), pandas as pd(DataFrame))\n",
        "\tNumber\n",
        "\t\t통계\n",
        "\t\t\tpd.describe() / pd.mean(), pd.median(), pd.mode() /\n",
        "\t\t\tpd.var(), pd.std() / pd.min(), pd.max(), pd.quantile() / pd.corr()\n",
        "\t\t소수점 : round(), ceil(), floor(), trunc()\n",
        "\t\t\t※ round() : 홀상짝하 ex) 3.5 -> 4 / 4.5 -> 4\n",
        "\tString\n",
        "\t\t포함 : startswith(), endswith(), contains()\n",
        "\t\t분할 : split()\n",
        "\t\t변환 : replace(), join(), strip(), upper(), lower(), swapcase()\n",
        "\tTime\n",
        "\t\t생성\n",
        "\t\t\tpd.to_datetime({Series}) # type을 'datetime64'로 바꾸기\n",
        "\t\t\tpd.date_range('start', periods = 6, freq = '3D') # 시간데이터 만들기\n",
        "\t\t추출\n",
        "\t\t\tpd.dt.date # YYYY-MM-DD(문자)\n",
        "\t\t\tpd.dt.year # 연(4자리 숫자)\n",
        "\t\t\tpd.dt.quarter # 분기(숫자)\n",
        "\t\t\tpd.dt.month # 월(숫자)\n",
        "\t\t\tpd.dt.day # 일(숫자)\n",
        "\t\t\tpd.dt.month_name() # 월명(문자)\n",
        "\t\t\tpd.dt.day_name() # 요일명(문자)\n",
        "\t\t\tpd.dt.weekday # 요일(숫자, 0:월, ..., 6:일)\n",
        "\t\t\tpd.dt.weekofyear # 연중주순(숫자)\n",
        "\t\t\tpd.dt.dayofyear # 연중일순(숫자)\n",
        "\t\t\tpd.dt.daysinmonth # 월간일수(숫자)\n",
        "\t\t\tpd.dt.time # HH:MM:SS(문자)\n",
        "\t\t\tpd.dt.hour # 시(숫자)\n",
        "\t\t\tpd.dt.minute # 분(숫자)\n",
        "\t\t\tpd.dt.second # 초(숫자)\n",
        "\tDataSet\n",
        "\t\t입출력 : pd.read_csv(), pd.to_csv()\n",
        "\t\t생성\n",
        "\t\t\tArray : np.array(), np.ones(), np.arange(), np.random.randn()\n",
        "\t\t\t\tnp.random.randint() : 정수 난수 생성\n",
        "\t\t\t\tnp.random.rand() : 0 ~ 1의 균일분포 난수 생성\n",
        "\t\t\t\tnp.random.randn() : Gaussian 표준정규분포(평균 0, 표준편차 1) 난수 생성\n",
        "\t\t\tSeries&DataFrame : pd.Series(), pd.DataFrame()\n",
        "\t\t확인\n",
        "\t\t\tFrame\n",
        "\t\t\t\tpd.index, pd.idxmax(), pd.idxmin() / pd.columns /\n",
        "\t\t\t\tpd.info(), pd.head(), pd.tail(), pd.dtypes, pd.shape\n",
        "\t\t\tValue\n",
        "\t\t\t\tpd.values / pd.size(), pd.count() / pd.unique(), pd.nunique(), pd.value_counts()\n",
        "\t\t\t\t※ size() : NaN 포함 / count() : NaN 제외\n",
        "\t\t변경\n",
        "\t\t\tnp&pd.where(), pd.drop() /\n",
        "\t\t\tpd.set_index(), pd.reset_index(), pd.sort_values(), pd.sort_index() /\n",
        "\t\t\tpd.values.tolist(), pd.index.to_series(), pd.to_frame(), pd.rename(), pd.astype() /\n",
        "\t\t\tnp.reshape(), np.ravel(), \"np.transpose()\" == \"np.T\"\n",
        "\t\t선별\n",
        "\t\t\tpd.loc[], pd.iloc[] / pd.rank(), pd.select_dtypes() / pd.cut(), pd.qcut() /\n",
        "\t\t\tpd.groupby(), pd.agg(), pd.apply()\n",
        "\t\t병합 : pd.concat(), pd.merge()\n",
        "\t\t결측치 : pd.isna(), pd.fillna()\n",
        "\tUDF(User Defined Function) : def ~ return\n",
        "- Statement\n",
        "\tConditional Statement : If ~ elif ~ else\n",
        "\tLoop Statement : For&While_Loop\n",
        "\n",
        "* Data Process\n",
        "- Filtration\n",
        "\tMissing&Invalid&Outler_Value Detection\n",
        "\t\tOutlier : x < Q1 - 1.5 × IQR or x > Q3 + 1.5 × IQR, IQR = Q3 - Q1\n",
        "\tSampling : PCA\n",
        "\tSmoothing\n",
        "- Transformation\n",
        "\tCategorization\n",
        "\tNormalization(Scaling)\n",
        "\t\tMin-Max : mm = (x - min(x)) / (max(x) - min(x))\n",
        "\t\tRoot&Log&Reciprocal Transformation\n",
        "\tStandardization : Z-Score\n",
        "\t\tZ-Score : zs = (x - mean(x)) / std(x)\n",
        "- Integration\n",
        "\tTrain&Test_Set\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LVcdTwklqRS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "### [ Type_2 ] ###\n",
        "##################\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "* Analysis Phase\n",
        "- Data Load > Data Preprocessing > Data Learning > Performance Evaluation > Visualization\n",
        "\n",
        "* Data Learning\n",
        "- Supervised Learning(Classification -> Category)\n",
        "\tKind : Decision Tree, Random Forest, KNN(K-Nearest Neighbor), SVM(Support Vector Machine)\n",
        "\t\t, Logistic Regression\n",
        "\tDesicion Tree -> Bagging&Ensemble -> Random Forest\n",
        "\tLogistic Regression : y = sigmoid(wx + b)\n",
        "- Supervised Learning(Regression(Prediction) -> Figure)\n",
        "\tKind : Decision Tree, Random Forest, Linear Regression\n",
        "\tLinear Regression\n",
        "\t\tSimple : y = ax + b\n",
        "\t\tMultiple : y = a1x1 + a2x2 + ... + anxn + b\n",
        "- Unsupervised Learning\n",
        "\tKind : Cluster_KMeans, Association_Apriori\n",
        "\tAssociation\n",
        "\t\tSupport(지지도) = P(A∩B)\n",
        "\t\tConfidence(신뢰도) = Support / P(A)\n",
        "\t\tLift(향상도) = Confidence / P(B)\n",
        "\n",
        "* Performance Evaluation\n",
        "- Distribution\n",
        "\tCentrality : Mean, Median, Mode\n",
        "\t\tMedian of Odd n -> (n + 1) / 2\n",
        "\t\tMedian of Even n -> (n/2 + (n/2 + 1)) / 2\n",
        "\tDispersion\n",
        "\t\tRange = Max(x) - Min(x)\n",
        "\t\tStandard Deviation = (V(x))^(1/2) = (Sig{i=1, n}{(xi - E(x))^2} / (n - 1))^(1/2)\n",
        "\t\tIQR = Q3 - Q1\n",
        "\tAsymmetry\n",
        "\t\tSkewness\n",
        "\t\t\tm3 > 0, Right Long Tail(Left Leaning) -> Mode < Median < Mean\n",
        "\t\t\tm3 = 0, Symmetry -> Mean = Median = Mode\n",
        "\t\t\tm3 < 0, Left Long Tail(Right Leaning) -> Mean < Median < Mode\n",
        "\t\tKurtosis\n",
        "\t\t\tm4 > 0, Leptokurtic(K > 3, sharp)\n",
        "\t\t\tm4 = 0, Mesokurtic(K = 3, Standard Normal Distribution)\n",
        "\t\t\tm4 < 0, Platykurtic(K < 3, blunt)\n",
        "- Classification\n",
        "\tContingency Table(Cancer(OX) × Exposure(OX) = a, b, c, d)\n",
        "\t\tRelative Risk(Prospective) = (a / (a + b)) / (c / (c + d))\n",
        "\t\tOdds Ratio(Retrospective) = Odds_1(Exposure O) / Odds_2(Exposure X) = (a / b) / (c / d)\n",
        "\tConfusion Matrix(Prediction(OX) × Action(OX) : TP, FN, FP, TN)\n",
        "\t\tAccuracy = (TP + TN) / All\n",
        "\t\tError Rate = (FP + FN) / All\n",
        "\t\tSensitivity(Recall) = TP / (TP + FN)\n",
        "\t\tSpecificity = TN / (TN + FP)\n",
        "\t\tPrecision = TP / (TP + FP)\n",
        "\t\tF1 Score = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\tROC & AUC\n",
        "\t\tROC Curve : y = TPR(Sensitivity), x = FPR = 1 - Specificity\n",
        "\t\tAUC(Area Under the Curve(ROC)) : 0.5 ~ 1.0, 0.9 ~ Excellent, 0.8 ~ Good\n",
        "- Regression\n",
        "\tCorrelation Coefficient : Rho = Cov(X, Y) / (Std(X) × Std(Y))\n",
        "\tR-Squared Score(결정계수) : 0(완전 비선형) ≤ R^2 = SSR / SST = 1 - (SSE / SST) ≤ 1(완전 선형)\n",
        "\t\tIndicator\n",
        "\t\t\ty_i : 실제값(관측값)   /   y_i↑{-} : 관측값의 평균   /   y_i↑{^} : 예측값(추정값)\n",
        "\t\t\tSST(Sum of Squares Total, 총제곱합) = Sig.{y_i - y_i↑{-}}^2\n",
        "\t\t\tSSE(Sum of Squares Error, 오차제곱합) = Sig.{y_i - y_i↑{^}}^2 : 설명 불가능한 변동\n",
        "\t\t\tSSR(Sum of Squares Regression, 회귀제곱합) = Sig.{y_i↑{^} - y_i↑{-}}^2 : 설명 가능한 변동\n",
        "\t\tMSE(Mean Squared Error) = (1 / n)Sig.{y_i - y_i↑{^}}^2\n",
        "\n",
        "* Visualization\n",
        "- ?\n",
        "\n",
        "! Line, Scatter(Bubble Chart), Histogram, Bar, Stem, Pie, Box, Error, Contour, 3D ~ Plot\n",
        "\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MS0Z69IFze3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "### [ Type_3 ] ###\n",
        "##################\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "* Hypothesis Test\n",
        "- Hypothesis\n",
        "\tH0(Null Hypothesis) : 통념적 믿음, '기각'을 지향\n",
        "\tH1(Alternative Hypothesis) : 새로운 입증, '채택'을 지향\n",
        "- Type Error\n",
        "\tType I Error(FP) : Reject the True H0\n",
        "\tType II Error(FN) : Accept the False H0\n",
        "- Significance Level&Probability\n",
        "\tDefinition\n",
        "\t\tSignificance Level(α, 유의수준) : Type I Error(FP) Limit\n",
        "\t\tSignificance Probability(P-value, 유의확률) : Support of H0\n",
        "\tComparison\n",
        "\t\tP-value < α : H0 기각\n",
        "\t\tP-value > α : H0 기각 불가\n",
        "\n",
        "* Parametric Significance Test(Test Statistic)\n",
        "- Population Mean\n",
        "\tT-Test\n",
        "\t\tOne-Sample T-Test : 한 집단의 평균과 기준값(모평균)과의 비교\n",
        "\t\tIndependent-Two-Sample T-Test : 독립된 두 집단의 평균 비교\n",
        "\t\tPaired-Two-Sample T-Test : 한 집단의 전후 평균 비교\n",
        "- Population Variance\n",
        "\tChi^2-Test : 단일표본 모분산 검정, 두 집단의 독립성 여부\n",
        "\tF-Test : 두 모분산 비 검정(ANOVA 중 일원분산분석)\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o0qaPooGzj64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @ Practice"
      ],
      "metadata": {
        "id": "DUmA98RHkiHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "### [ Type_1 ] ###\n",
        "##################\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "# ?\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dnmYH588tMPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################################\n",
        "\n",
        "'''\n",
        "* Practice\n",
        "- Type_1\n",
        "\twhere()\n",
        "\t\tdf1['W'] = df1['N'].where(df1['N'] < 400, 400).to_frame()\n",
        "\t\tdf2['W'] = np.where(df2['N'] % 3 == 0, 'T', 'F')\n",
        "\t{28} if + boolean\n",
        "\t{31} for, while + operator, {35} list\n",
        "\t{37} Tuple&Dictionary\n",
        "\t'구분자'.join(리스트)\n",
        "\t{41} UDF\n",
        "\t{42} !pip list\n",
        "\n",
        "df = pd.DataFrame(np.random.randint(0,100,size=(100, 5)), columns=list(\"abcde\"))\n",
        "\n",
        "rand(), randn(), 문자리스트 생성방법\n",
        "letters_set = string.ascii_letters + string.digits\n",
        "uppercases = string.ascii_uppercase\n",
        "letters_set = string.ascii_lowercase\n",
        "random_list = random.sample(letters_set,10)\n",
        "\n",
        "데이터프레임 전체 컬럼을 같은 값으로 채우기\n",
        "# 1\n",
        "df3 = pd.DataFrame(np.random.randint(0, 100, size = (100, 5)), columns = list('abcde'))\n",
        "df3.loc[:] = np.nan\n",
        "df3 = df3.fillna(1)\n",
        "# 2\n",
        "df4 = pd.DataFrame(np.array([[1 for j in range(5)] for i in range(10)]))\n",
        "\n",
        "list -> array\n",
        "my_list = [2,4,6,8,10]\n",
        "my_array = np.array(my_list)\n",
        "\n",
        "array -> list\n",
        "my_array = np.random.rand(4)\n",
        "re_list = my_array.tolist()\n",
        "\n",
        "array -> dataframe\n",
        "\n",
        "dataframe -> array\n",
        "\n",
        "ds = sns.load_dataset('titanic')\n",
        "ds.describe(include = 'all')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "seaborn_titanic -> visualize\n",
        "\n",
        "{183} K-menas for loop\n",
        "\n",
        "- Type_2\n",
        "\n",
        "Line, Scatter(Bubble Chart), Histogram, Bar, Stem, Pie, Box, Error, Contour, 3D ~ Plot\n",
        "\n",
        "- Type_3\n",
        "'''"
      ],
      "metadata": {
        "id": "8hBPP7N4zw1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "### [ Practice(Type I) ] ###\n",
        "############################\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "* Setup\n",
        "- Import\n",
        "\timport numpy as np\n",
        "\timport pandas as pd\n",
        "\timport seaborn as sns\n",
        "- Display\n",
        "\tpd.set_option('display.max_columns', None)\n",
        "\n",
        "* DataFrame\n",
        "- Generation\n",
        "\tdf1 = pd.DataFrame({'N' : [123, 345, 567],\n",
        "\t\t\t\t\t\t'A' : ['a1', 'a2', 'a3'],\n",
        "\t\t\t\t\t\t'B' : ['b1', 'b2', 'b3']},\n",
        "\t\t\t\t\t\tindex = [0, 1, 2])\n",
        "\tdf2 = pd.DataFrame([[123, 'b1', 'c1'],\n",
        "\t\t\t\t\t\t[345, 'b2', 'c2'],\n",
        "\t\t\t\t\t\t[248, 'b3', 'c3']],\n",
        "\t\t\t\t\t\tindex = [1, 2, 3],\n",
        "\t\t\t\t\t\tcolumns = ['N', 'B', 'C'])\n",
        "\tsr1 = pd.Series(['c1', 'c2', 'c4', 'c5'], name = 'C', index = [1, 2, 3, 4])\n",
        "- Addition\n",
        "\tdf1['C'] = np.NaN\n",
        "\tdf1.loc[3] = np.NaN\n",
        "- Deletion\n",
        "\tdf1.drop(3, inplace = True)\n",
        "\tdel df1['C']\n",
        "- Merger\n",
        "\tdf_m1 = pd.concat([df1, df2]).reset_index(drop = False)\n",
        "\tdf_m2 = pd.merge(df2, sr1).sort_index(ascending = False) # common column ['C'] inner join\n",
        "\tdf_m3 = pd.merge(df1, df2, how = 'outer', on = 'N').sort_values('N').fillna(-99)\n",
        "- Group\n",
        "\tdata = sns.load_dataset('titanic')\n",
        "\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\tdata['age_cut'] = pd.cut(data['age'], bins = list(range(0, 81, 10))) # The Equal Length\n",
        "\tdata['age_qcut'] = pd.qcut(data['age'], 3, labels = ['child', 'young', 'old']) # The Equal Number\n",
        "\tdata[['age_cut', 'age_qcut']].value_counts().unstack()\n",
        "\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\tdata[data.alone == True].groupby('class', as_index = False)['fare'].agg(['mean', 'var', 'max', 'min'])\n",
        "\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\tdfgp = data[['embark_town', 'class']].groupby(['embark_town', 'class']).size().unstack() # dfgp : data frame group\n",
        "\tdfgp.loc[:, :] = (dfgp.values / dfgp.sum(axis = 1).values.reshape(-1, 1))\n",
        "- Pivot Table\n",
        "\tdata = sns.load_dataset('titanic')\n",
        "\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\tf_mean = data.fare.mean()\n",
        "\tf_std = data.fare.std()\n",
        "\tdef check(x) :\n",
        "\t\tif x.sex == 'female' and x.alone == True :\n",
        "\t\t\treturn 'A'\n",
        "\t\telif x.fare < (f_mean - 1.5 * f_std) or x.fare > (f_mean + 1.5 * f_std) :\n",
        "\t\t\treturn 'B'\n",
        "\t\telse :\n",
        "\t\t\treturn 'C'\n",
        "\tdata['state'] = data.apply(check, axis = 1)\n",
        "\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\tdata.pivot_table(index = 'class', columns = 'state', values = 'fare', margins = True, aggfunc = ['sum', 'mean'])\n",
        "\n",
        "* Time & String\n",
        "- Time\n",
        "\tdf = pd.read_csv('/content/drive/MyDrive/BAE/Datasets/Air_Pollution_in_Seoul.csv', index_col = 0).reset_index(drop = False)\n",
        "\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\tdtt = pd.DataFrame(df['Measurement_Date']).rename(columns = {'Measurement_Date' : 'MD'}) # dtt : date time table\n",
        "\tdtt['MD'] = pd.to_datetime(dtt['MD'])\n",
        "\tdtt['CD'] = pd.date_range('2017-01-01', periods = 25904, freq = '61T') # CD : Comparison_Date\n",
        "\tdtt['DD'] = (dtt.CD - dtt.MD).apply(lambda x : x.days * 86400 + x.seconds) # DD : Date_Difference\n",
        "\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\tdtt['PP'] = df['PM10'].diff().fillna(method = 'bfill') # PP : Processed_PM10\n",
        "\tdtt['PP'] = dtt['PP'].rolling(3).sum().fillna(-99)\n",
        "\tq3 = dtt['PP'].quantile(.75)\n",
        "\tdtt['SP'] = df['PM10'].apply(lambda x : 1 if x >= q3 else 0) # SP : Selected_PM10\n",
        "\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\tdtt.set_index('MD', inplace = True, drop = True)\n",
        "\tresult = dtt.DD.resample('YS').agg(['mean', 'std'])\n",
        "- String\n",
        "\tdtt['SD'] = dtt['CD'].apply(lambda x : x.strftime(\"%Y-%m-%d %H:%M:%S / %A of %B\")) # SD : String_Date\n",
        "\tdtt['SD'] = dtt.SD.apply(lambda x : x.replace('a', 'b', 1).strip('0123456'))\n",
        "\tdtt['SD'] = dtt['SD'].str.split(\"/\", expand = True)[1].apply(lambda x : x.upper())\n",
        "\tresult = dtt[dtt.SD.str.contains('FRI', na = False)].drop_duplicates('SD', keep = 'last')\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o8Ykj6vg3gVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @ Practice(Type II)"
      ],
      "metadata": {
        "id": "3h_aSg2t0BTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "### [ Practice(Type II) : Classification ] ###\n",
        "##############################################\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "'''1. Load&Check Datasets'''\n",
        "\n",
        "DataPath = '/content/drive/MyDrive/BAE/Datasets/RegularPractice/'\n",
        "X_train = pd.read_csv(DataPath + 'titanic3_X_train.csv', encoding = 'cp949')\n",
        "X_test = pd.read_csv(DataPath + 'titanic3_X_test.csv', encoding = 'cp949')\n",
        "y_train = pd.read_csv(DataPath + 'titanic3_y_train.csv', encoding = 'cp949')\n",
        "\n",
        "# print(X_train.info(), X_test.info(), y_train.info())\n",
        "# print('X_train : \\n', X_train.describe(), '\\n\\nX_test : \\n', X_test.describe(), '\\n\\ny_train : \\n', y_train.describe(), '\\n')\n",
        "# print('X_TRAIN : \\n', X_TRAIN, '\\n\\nX_VAL : \\n', X_VAL, '\\n\\ny_TRAIN : \\n', y_TRAIN, '\\n\\ny_VAL : \\n', y_VAL, '\\n')\n",
        "# print('X_train : \\n', X_train, '\\n\\nX_test : \\n', X_test, '\\n\\ny_train : \\n', y_train, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "'''2-1. [ Pre-Processing ] Delete Unnecessary Columns'''\n",
        "\n",
        "ID = X_test['ID'].copy()\n",
        "X_train.drop(columns = ['ID', 'name'], inplace = True)\n",
        "X_test.drop(columns = ['ID', 'name'], inplace = True)\n",
        "y_train.drop(columns = 'ID', inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "'''2-2. [ Pre-Processing ] Process The Missing Values'''\n",
        "\n",
        "# print(X_train.isna().sum())\n",
        "# print(X_test.isna().sum())\n",
        "\n",
        "\n",
        "\n",
        "cond_na = X_train['age'].isna()\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# print(pearsonr(y_train['survived'][~cond_na], X_train['age'][~cond_na]))\n",
        "\n",
        "X_train.drop('age', axis = 1, inplace = True)\n",
        "X_test.drop('age', axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "f_mean = X_train['fare'].mean().round(4)\n",
        "\n",
        "X_train['fare'] = X_train['fare'].fillna(f_mean)\n",
        "\n",
        "\n",
        "\n",
        "cond_na300 = (X_train.isna().sum() >= 300) # 'cabin' column\n",
        "colnm_na300 = X_train.columns[cond_na300]\n",
        "\n",
        "X_train.drop(colnm_na300, axis = 1, inplace = True)\n",
        "X_test.drop(colnm_na300, axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "e_mode = X_train['embarked'].value_counts().idxmax()\n",
        "\n",
        "X_train['embarked'] = X_train['embarked'].fillna(e_mode)\n",
        "X_test['embarked'] = X_test['embarked'].fillna(e_mode)\n",
        "\n",
        "\n",
        "\n",
        "'''2-3. [ Pre-Processing ] Process The Categorical Types'''\n",
        "\n",
        "# print(X_train.select_dtypes('object').nunique())\n",
        "# print(X_test.select_dtypes('object').nunique())\n",
        "\n",
        "\n",
        "\n",
        "# print(X_train['sex'].value_counts())\n",
        "# print(X_test['sex'].value_counts())\n",
        "\n",
        "X_train['sex'] = X_train['sex'].map({'male' : 'male', 'female' : 'female', 'F' : 'female'})\n",
        "X_test['sex'] = X_test['sex'].map({'male' : 'male', 'female' : 'female', 'F' : 'female'})\n",
        "\n",
        "\n",
        "\n",
        "X_train.drop('ticket', axis = 1, inplace = True)\n",
        "X_test.drop('ticket', axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "'''2-4. [ Pre-Processing ] Process The Numerical Types(Check Correlation)'''\n",
        "\n",
        "X_train['pclass_gp'] = X_train['pclass'].astype('object')\n",
        "X_test['pclass_gp'] = X_test['pclass'].astype('object')\n",
        "\n",
        "X_train.drop('pclass', axis = 1, inplace = True)\n",
        "X_test.drop('pclass', axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "X_train['fam'] = X_train['sibsp'] + X_train['parch']\n",
        "X_test['fam'] = X_test['sibsp'] + X_test['parch']\n",
        "\n",
        "X_train.drop(['sibsp', 'parch'], axis = 1, inplace = True)\n",
        "X_test.drop(['sibsp', 'parch'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "'''2-5. [ Pre-Processing ] Data Separation'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train, y_train, random_state = 1234, test_size = 0.1)\n",
        "\n",
        "# print(X_TRAIN.shape)\n",
        "# print(X_VAL.shape)\n",
        "# print(y_TRAIN.shape)\n",
        "# print(y_VAL.shape)\n",
        "\n",
        "\n",
        "\n",
        "'''2-6. [ Pre-Processing ] Encoding'''\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X_TRAIN_category = X_TRAIN.select_dtypes('object').copy()\n",
        "X_VAL_category = X_VAL.select_dtypes('object').copy()\n",
        "X_TEST_category = X_test.select_dtypes('object').copy()\n",
        "\n",
        "enc = OneHotEncoder(sparse_output = False).fit(X_TRAIN_category)\n",
        "\n",
        "X_TRAIN_OH = enc.transform(X_TRAIN_category)\n",
        "X_VAL_OH = enc.transform(X_VAL_category)\n",
        "X_TEST_OH = enc.transform(X_TEST_category)\n",
        "\n",
        "\n",
        "\n",
        "'''2-7. [ Pre-Processing ] Scaling'''\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X_TRAIN_conti = X_TRAIN.select_dtypes(exclude = 'object').copy()\n",
        "X_VAL_conti = X_VAL.select_dtypes(exclude = 'object').copy()\n",
        "X_TEST_conti = X_test.select_dtypes(exclude = 'object').copy()\n",
        "\n",
        "scale = MinMaxScaler().fit(X_TRAIN_conti)\n",
        "\n",
        "X_TRAIN_STD = scale.transform(X_TRAIN_conti)\n",
        "X_VAL_STD = scale.transform(X_VAL_conti)\n",
        "X_TEST_STD = scale.transform(X_TEST_conti)\n",
        "\n",
        "\n",
        "\n",
        "'''2-8. [ Pre-Processing ] Concatenating'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD], axis = 1)\n",
        "X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD], axis = 1)\n",
        "\n",
        "y_TRAIN = y_TRAIN.values.ravel()\n",
        "y_VAL = y_VAL.values.ravel()\n",
        "\n",
        "\n",
        "\n",
        "'''3-1. [ Modeling ] Train'''\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators = 500,\n",
        "                            max_depth = 30,\n",
        "                            min_samples_leaf = 10,\n",
        "                            random_state = 2023)\n",
        "\n",
        "model_rf = rf.fit(X_TRAIN, y_TRAIN)\n",
        "\n",
        "\n",
        "\n",
        "'''3-2. [ Modeling ] Validation'''\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "pred_val = model_rf.predict(X_VAL)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_VAL, pred_val, pos_label = 1)\n",
        "auc_val = auc(fpr, tpr)\n",
        "print('auc_val : ', auc_val, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "'''3-3. [ Modeling ] Test'''\n",
        "\n",
        "X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD], axis = 1)\n",
        "\n",
        "pred_test = model_rf.predict(X_TEST)\n",
        "\n",
        "\n",
        "\n",
        "'''4. Submit&Check Result'''\n",
        "\n",
        "obj = {'ID' : ID,\n",
        "       'survived' : pred_test}\n",
        "result = pd.DataFrame(obj)\n",
        "\n",
        "ResultPath = '/content/drive/MyDrive/BAE/Result/'\n",
        "result.to_csv(ResultPath + 'cp.csv', index = False) # cp : ClassificationPractice\n",
        "\n",
        "ResultCheck = pd.read_csv(ResultPath + 'cp.csv', encoding = 'cp949')\n",
        "print('ResultCheck : \\n', ResultCheck, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "'''5(Ext). Result Evaluation'''\n",
        "\n",
        "y_test = pd.read_csv(DataPath + 'titanic3_y_test.csv', encoding = 'cp949')\n",
        "y_test = y_test['survived'].ravel()\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, pred_test, pos_label = 1)\n",
        "auc_test = auc(fpr, tpr)\n",
        "print('auc_test : ', auc_test, '\\n')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UIq_pMCa3leQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "### [ Practice(Type II) : Regression ] ###\n",
        "##########################################\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "'''1. Load&Check Datasets'''\n",
        "\n",
        "DataPath = '/content/drive/MyDrive/BAE/Datasets/RegularPractice/'\n",
        "X_train = pd.read_csv(DataPath + 'FIFA_X_train.csv', encoding = 'cp949')\n",
        "X_test = pd.read_csv(DataPath + 'FIFA_X_test.csv', encoding = 'cp949')\n",
        "y_train = pd.read_csv(DataPath + 'FIFA_y_train.csv', encoding = 'cp949')\n",
        "\n",
        "# print(X_train.info(), X_test.info(), y_train.info())\n",
        "# print('X_train : \\n', X_train.describe(), '\\n\\nX_test : \\n', X_test.describe(), '\\n\\ny_train : \\n', y_train.describe(), '\\n')\n",
        "# print('X_TRAIN : \\n', X_TRAIN, '\\n\\nX_VAL : \\n', X_VAL, '\\n\\ny_TRAIN : \\n', y_TRAIN, '\\n\\ny_VAL : \\n', y_VAL, '\\n')\n",
        "# print('X_train : \\n', X_train, '\\n\\nX_test : \\n', X_test, '\\n\\ny_train : \\n', y_train, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "'''2-1. [ Pre-Processing ] Delete Unnecessary Columns'''\n",
        "\n",
        "ID = X_test['ID'].copy()\n",
        "X_train.drop(columns = 'ID', inplace = True)\n",
        "X_test.drop(columns = 'ID', inplace = True)\n",
        "y_train.drop(columns = 'ID', inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "'''2-2. [ Pre-Processing ] Process The Missing Values'''\n",
        "\n",
        "# print(X_train.isna().sum())\n",
        "# print(X_test.isna().sum())\n",
        "\n",
        "\n",
        "\n",
        "X_train['Position_Class'] = X_train['Position_Class'].fillna('unknown')\n",
        "X_test['Position_Class'] = X_test['Position_Class'].fillna('unknown')\n",
        "\n",
        "# print(X_train['Position_Class'].value_counts())\n",
        "# print(X_test['Position_Class'].value_counts())\n",
        "\n",
        "PC_train = X_train['Position_Class'].copy()\n",
        "PC_test = X_test['Position_Class'].copy()\n",
        "\n",
        "mip = ['LF', 'CM', 'RDM', 'RWB', 'GK'] # mip : missing individual position\n",
        "mpc = ['Forward', 'Midfielder', 'Defender', 'Defender', 'GoalKeeper'] # mpc : misisng position class\n",
        "\n",
        "for r, s in zip(mip, mpc) :\n",
        "    PC_train[X_train['Position'] == r] = s\n",
        "    PC_test[X_test['Position'] == r] = s\n",
        "\n",
        "X_train['Position_Class'] = PC_train\n",
        "X_test['Position_Class'] = PC_test\n",
        "\n",
        "pd.crosstab(index = X_train['Position'], columns = X_train['Position_Class'])\n",
        "\n",
        "X_train.drop(columns = 'Position', inplace = True)\n",
        "X_test.drop(columns = 'Position', inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "h_train = X_train['Height'].copy()\n",
        "hc_train = X_train['Height_cm'].copy()\n",
        "\n",
        "hs = h_train.str.split(\"'\", expand = True).astype('float64') # hs : height split\n",
        "\n",
        "hc_train = hc_train.fillna(hs[0] * 30 + hs[1] * 2.5)\n",
        "X_train['Height_cm'] = hc_train\n",
        "\n",
        "h_test = X_test['Height'].copy()\n",
        "hc_test = X_test['Height_cm'].copy()\n",
        "\n",
        "hs = h_test.str.split(\"'\", expand = True).astype('float64') # hs : height split\n",
        "\n",
        "hc_test = hc_test.fillna(hs[0] * 30 + hs[1] * 2.5)\n",
        "X_test['Height_cm'] = hc_test\n",
        "\n",
        "X_train.drop(columns = 'Height', inplace = True)\n",
        "X_test.drop(columns = 'Height', inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "w_mean = X_train['Weight_lb'].mean()\n",
        "\n",
        "X_train['Weight_lb'] = X_train['Weight_lb'].fillna(w_mean)\n",
        "\n",
        "\n",
        "\n",
        "'''2-3. [ Pre-Processing ] Process The Categorical Types'''\n",
        "\n",
        "# print(X_train.select_dtypes('object').nunique())\n",
        "# print(X_test.select_dtypes('object').nunique())\n",
        "\n",
        "\n",
        "\n",
        "X_train['Age_gp'] = X_train['Age'].str[0]\n",
        "X_test['Age_gp'] = X_test['Age'].str[0]\n",
        "\n",
        "X_train.drop('Age', axis = 1, inplace = True)\n",
        "X_test.drop('Age', axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "X_train.drop(columns = 'Club', inplace = True)\n",
        "X_test.drop(columns = 'Club', inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "wr_train = X_train['Work_Rate'].copy()\n",
        "wr_train = wr_train.str.replace(' ', '')\n",
        "\n",
        "X_train['WR_Attack'] = wr_train.str.split(\"/\", expand = True)[0]\n",
        "X_train['WR_Defend'] = wr_train.str.split(\"/\", expand = True)[1]\n",
        "\n",
        "wr_test = X_test['Work_Rate'].copy()\n",
        "wr_test = wr_test.str.replace(' ', '')\n",
        "\n",
        "X_test['WR_Attack'] = wr_test.str.split(\"/\", expand = True)[0]\n",
        "X_test['WR_Defend'] = wr_test.str.split(\"/\", expand = True)[1]\n",
        "\n",
        "X_train.drop(columns = 'Work_Rate', inplace = True)\n",
        "X_test.drop(columns = 'Work_Rate', inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "'''2-4. [ Pre-Processing ] Process The Numerical Types(Check Correlation)'''\n",
        "\n",
        "X_train.drop('Jersey_Number', axis = 1, inplace = True)\n",
        "X_test.drop('Jersey_Number', axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "# print(X_train['Contract_Valid_Until'].sort_values().unique())\n",
        "# print(X_test['Contract_Valid_Until'].sort_values().unique())\n",
        "\n",
        "X_train['CVU_gp'] = X_train['Contract_Valid_Until'].astype('object')\n",
        "X_test['CVU_gp'] = X_test['Contract_Valid_Until'].astype('object')\n",
        "\n",
        "X_train.drop('Contract_Valid_Until', axis = 1, inplace = True)\n",
        "X_test.drop('Contract_Valid_Until', axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "colnm_conti = ['Overall', 'Height_cm', 'Weight_lb', 'Release_Clause', 'Wage']\n",
        "X_train[colnm_conti].corr() # Wage vs Release_Clause\n",
        "\n",
        "X_train.drop('Release_Clause', axis = 1, inplace = True)\n",
        "X_test.drop('Release_Clause', axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "'''2-5. [ Pre-Processing ] Data Separation'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_TRAIN, X_VAL, y_TRAIN, y_VAL = train_test_split(X_train, y_train, random_state = 1234, test_size = 0.3)\n",
        "\n",
        "# print(X_TRAIN.shape)\n",
        "# print(X_VAL.shape)\n",
        "# print(y_TRAIN.shape)\n",
        "# print(y_VAL.shape)\n",
        "\n",
        "\n",
        "\n",
        "'''2-6. [ Pre-Processing ] Encoding'''\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X_TRAIN_category = X_TRAIN.select_dtypes('object').copy()\n",
        "X_VAL_category = X_VAL.select_dtypes('object').copy()\n",
        "X_TEST_category = X_test.select_dtypes('object').copy()\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False).fit(X_TRAIN_category)\n",
        "\n",
        "X_TRAIN_OH = enc.transform(X_TRAIN_category)\n",
        "X_VAL_OH = enc.transform(X_VAL_category)\n",
        "X_TEST_OH = enc.transform(X_TEST_category)\n",
        "\n",
        "\n",
        "\n",
        "'''2-7. [ Pre-Processing ] Scaling'''\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "colnm_conti = ['Overall', 'Height_cm', 'Weight_lb', 'Wage']\n",
        "\n",
        "X_TRAIN_conti = X_TRAIN[colnm_conti].copy()\n",
        "X_VAL_conti = X_VAL[colnm_conti].copy()\n",
        "X_TEST_conti = X_test[colnm_conti].copy()\n",
        "\n",
        "scale = StandardScaler().fit(X_TRAIN_conti)\n",
        "\n",
        "X_TRAIN_STD = scale.transform(X_TRAIN_conti)\n",
        "X_VAL_STD = scale.transform(X_VAL_conti)\n",
        "X_TEST_STD = scale.transform(X_TEST_conti)\n",
        "\n",
        "\n",
        "\n",
        "'''2-8. [ Pre-Processing ] Concatenating'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X_TRAIN = np.concatenate([X_TRAIN_OH, X_TRAIN_STD], axis = 1)\n",
        "X_VAL = np.concatenate([X_VAL_OH, X_VAL_STD], axis = 1)\n",
        "\n",
        "y_TRAIN = y_TRAIN.values.ravel()\n",
        "y_VAL = y_VAL.values.ravel()\n",
        "\n",
        "\n",
        "\n",
        "'''3-1. [ Modeling ] Train'''\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 500,\n",
        "                           max_depth = 30,\n",
        "                           min_samples_leaf = 10,\n",
        "                           random_state = 2023)\n",
        "\n",
        "model_rf = rf.fit(X_TRAIN, y_TRAIN)\n",
        "\n",
        "\n",
        "\n",
        "'''3-2. [ Modeling ] Validation'''\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "pred_val = model_rf.predict(X_VAL)\n",
        "\n",
        "rmse_val = mean_squared_error(y_VAL, pred_val, squared = False)\n",
        "print('rmse_val : ', rmse_val, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "'''3-3. [ Modeling ] Test'''\n",
        "\n",
        "X_TEST = np.concatenate([X_TEST_OH, X_TEST_STD], axis = 1)\n",
        "\n",
        "pred_test = model_rf.predict(X_TEST)\n",
        "\n",
        "\n",
        "\n",
        "'''4. Submit&Check Result'''\n",
        "\n",
        "obj = {'ID' : ID,\n",
        "       'Purchased' : pred_test}\n",
        "result = pd.DataFrame(obj)\n",
        "\n",
        "ResultPath = '/content/drive/MyDrive/BAE/Result/'\n",
        "result.to_csv(ResultPath + 'rp.csv', index = False) # rp : RegressionPractice\n",
        "\n",
        "ResultCheck = pd.read_csv(ResultPath + 'rp.csv', encoding = 'cp949')\n",
        "print('ResultCheck : \\n', ResultCheck, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "'''5(Ext). Result Evaluation'''\n",
        "\n",
        "y_test = pd.read_csv(DataPath + 'FIFA_y_test.csv', encoding = 'cp949')\n",
        "y_test = y_test['Value'].ravel()\n",
        "\n",
        "rmse_test = mean_squared_error(y_test, pred_test, squared = False)\n",
        "print('rmse_test : ', rmse_test, '\\n')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-_sqnJ8DZhZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @ Practice(Type III)"
      ],
      "metadata": {
        "id": "3hGlaptu0H9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "### [ Type III : T-Test & ANOVA(F-Test) ] ###\n",
        "#############################################\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DataPath = '/content/drive/MyDrive/BAE/Datasets/RegularPractice/blood_pressure.csv'\n",
        "data = pd.read_csv(DataPath, index_col = 0)\n",
        "\n",
        "\n",
        "\n",
        "'''1. Sample Mean'''\n",
        "\n",
        "bm = data['bp_before'].mean().round(2) # bm : before_mean\n",
        "am = data['bp_after'].mean().round(2) # am : after_mean\n",
        "sm = np.round(am - bm, 2) # sm : sample_mean\n",
        "\n",
        "print('1 Sample Mean : ', am)\n",
        "print('2 Sample Mean : ', sm)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "'''2. Test Statistics'''\n",
        "\n",
        "import scipy\n",
        "from scipy import stats\n",
        "\n",
        "ttest_1samp = scipy.stats.ttest_1samp(data['bp_after'], popmean = 160, alternative = 'less') # popmean : H0의 기댓값\n",
        "ttest_ind = scipy.stats.ttest_ind(data['bp_after'], data['bp_before'], equal_var = True, alternative = 'less')\n",
        "ttest_rel = scipy.stats.ttest_rel(data['bp_after'], data['bp_before'], alternative = 'less') # less : H1 < H0\n",
        "anova = scipy.stats.f_oneway(data['bp_after'], data['bp_before'])\n",
        "\n",
        "print('T-Test(One-Sample) : ', ttest_1samp)\n",
        "print('T-Test(Independent-Two-Sample) : ', ttest_ind)\n",
        "print('T-Test(Paired-Two-Sample) : ', ttest_rel)\n",
        "print('ANOVA t-value : ', anova[0].round(4)) # anova.statistic\n",
        "print('ANOVA p-value : ', anova[1].round(4)) # anova.pvalue\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "'''3. Accept or Reject H0'''\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "cv1 = scipy.stats.t.ppf(q = alpha, df = len(data) - 1) # 1 Sample\n",
        "cv2 = scipy.stats.t.ppf(q = alpha, df = 2 * len(data) - 2) # 2 Sample\n",
        "cv3 = scipy.stats.f.ppf(q = 1 - alpha, dfn = 1, dfd = len(data) - 2) # ANOVA\n",
        "\n",
        "print('Critical Value(1-Sample) : ', cv1)\n",
        "print('Critical Value(2-Sample) : ', cv2)\n",
        "print('Critical Value(ANOVA) : ', cv3)\n",
        "print()\n",
        "\n",
        "# if ttest_rel[0] < cv2 :\n",
        "if anova[0] > cv3 :\n",
        "    print(\"H0 is rejected under 0.05 (Critical Value)\")\n",
        "else :\n",
        "    print(\"H0 is NOT rejected under 0.05 (Critical Value)\")\n",
        "\n",
        "if anova[1] < alpha :\n",
        "    print(\"H0 is rejected under 0.05 (Significance Level)\")\n",
        "else :\n",
        "    print(\"H0 is NOT rejected under 0.05 (Significance Level)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oTMkcy2IQSy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "### [ Type III : Chi^2-Test ] ###\n",
        "#################################\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "ds = sns.load_dataset('titanic')\n",
        "\n",
        "\n",
        "\n",
        "'''1. Contingency Table'''\n",
        "\n",
        "ct = pd.crosstab(ds['sex'], ds['survived'])\n",
        "print('Table : \\n', ct, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "'''2. Test Statistics'''\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "chi2 = chi2_contingency(ct)\n",
        "print('chi2 : ', chi2)\n",
        "print('t-value : ', chi2[0]) # chi2.statistic\n",
        "print('p-value : ', chi2[1]) # chi2.pvalue\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "'''3. Odds'''\n",
        "\n",
        "from statsmodels.formula.api import logit\n",
        "\n",
        "logit('survived~sex+sibsp+parch+fare', data = ds).fit().summary() # Intercept : 절편 /// coef : ln(Odds)\n",
        "# parch 변수의 계수 값 : -0.2007\n",
        "\n",
        "np.exp(logit('survived~sex+sibsp+parch+fare', data = ds).fit().params) # Odds = np.exp(Logit Values)\n",
        "# sibsp가 한 단위 증가할 때 생존할 Odds Ratio : ?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ihwxatFxX_HA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}